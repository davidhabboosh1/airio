{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApA2WBvW4qjB"
      },
      "source": [
        "# AirIO Train with WMT Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xvFqesO4hSk"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import functools\n",
        "import tempfile\n",
        "\n",
        "import airio\n",
        "from clu.data import dataset_iterator as clu_dataset_iterator\n",
        "from seqio import vocabularies\n",
        "from t5x import adafactor\n",
        "from t5x import gin_utils\n",
        "from t5x import models\n",
        "from t5x import partitioning\n",
        "from t5x import train as train_lib\n",
        "from t5x import trainer\n",
        "from t5x import utils\n",
        "from t5x.examples.t5 import network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cgIEkge6F9_"
      },
      "outputs": [],
      "source": [
        "_DEFAULT_EXTRA_IDS = 100\n",
        "_DEFAULT_SPM_PATH = \"gs://t5-data/vocabs/cc_all.32000/sentencepiece.model\"\n",
        "_DEFAULT_VOCAB = vocabularies.SentencePieceVocabulary(\n",
        "    _DEFAULT_SPM_PATH, _DEFAULT_EXTRA_IDS\n",
        ")\n",
        "_SOURCE_SEQUENCE_LENGTH = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x93-Hpn46REh"
      },
      "outputs": [],
      "source": [
        "def create_dataset(\n",
        "    task: airio.dataset_providers.Task,\n",
        ") -\u003e clu_dataset_iterator.DatasetIterator:\n",
        "  sequence_lengths = {\n",
        "      \"inputs\": _SOURCE_SEQUENCE_LENGTH,\n",
        "      \"targets\": _SOURCE_SEQUENCE_LENGTH,\n",
        "  }\n",
        "  return task.get_dataset(\n",
        "      sequence_lengths,\n",
        "      \"train\",\n",
        "      shuffle=False,\n",
        "      feature_converter=airio.feature_converters.PyGrainEncDecFeatureConverter(),\n",
        "      batch_size=8,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw8sQf2f6TiD"
      },
      "outputs": [],
      "source": [
        "def get_t5_model(**config_overrides) -\u003e models.EncoderDecoderModel:\n",
        "  \"\"\"Returns a small T5 1.1 model to use for testing.\"\"\"\n",
        "  tiny_config = network.T5Config(\n",
        "      vocab_size=32128,\n",
        "      dtype=\"bfloat16\",\n",
        "      emb_dim=8,\n",
        "      num_heads=4,\n",
        "      num_encoder_layers=2,\n",
        "      num_decoder_layers=2,\n",
        "      head_dim=3,\n",
        "      mlp_dim=16,\n",
        "      mlp_activations=(\"gelu\", \"linear\"),\n",
        "      dropout_rate=0.0,\n",
        "      logits_via_embedding=False,\n",
        "  )\n",
        "\n",
        "  tiny_config = dataclasses.replace(tiny_config, **config_overrides)\n",
        "  return models.EncoderDecoderModel(\n",
        "      module=network.Transformer(tiny_config),\n",
        "      input_vocabulary=_DEFAULT_VOCAB,\n",
        "      output_vocabulary=_DEFAULT_VOCAB,\n",
        "      optimizer_def=adafactor.Adafactor(\n",
        "          decay_rate=0.8,\n",
        "          step_offset=0,\n",
        "          logical_factor_rules=adafactor.standard_logical_factor_rules(),\n",
        "      ),\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji9ERpI76WHk"
      },
      "outputs": [],
      "source": [
        "def create_train_fn(task: airio.dataset_providers.Task):\n",
        "  \"\"\"Returns a function for training.\"\"\"\n",
        "  train_dataset_cfg = utils.DatasetConfig(\n",
        "      mixture_or_task_name=task,\n",
        "      task_feature_lengths={\"inputs\": 32, \"targets\": 32},\n",
        "      split=\"train\",\n",
        "      batch_size=8,\n",
        "      shuffle=False,\n",
        "      pack=False,\n",
        "      use_cached=False,\n",
        "      seed=0,\n",
        "  )\n",
        "  eval_dataset_cfg = utils.DatasetConfig(\n",
        "      mixture_or_task_name=task,\n",
        "      task_feature_lengths={\"inputs\": 32, \"targets\": 32},\n",
        "      split=\"validation\",\n",
        "      batch_size=8,\n",
        "      shuffle=False,\n",
        "      pack=False,\n",
        "      use_cached=False,\n",
        "      seed=0,\n",
        "  )\n",
        "  partitioner = partitioning.PjitPartitioner(num_partitions=4)\n",
        "  trainer_cls = functools.partial(\n",
        "      trainer.Trainer,\n",
        "      learning_rate_fn=utils.create_learning_rate_scheduler(\n",
        "          factors=\"constant * rsqrt_decay\",\n",
        "          base_learning_rate=1.0,\n",
        "          warmup_steps=1000,\n",
        "      ),\n",
        "      num_microbatches=None,\n",
        "  )\n",
        "  restore_cfg = None\n",
        "  ckpt_cfg = utils.CheckpointConfig(\n",
        "      save=utils.SaveCheckpointConfig(\n",
        "          dtype=\"float32\",\n",
        "          period=4,\n",
        "          checkpoint_steps=[0, 1, 2, 3, 4, 80, 97, 100],\n",
        "      ),\n",
        "      restore=restore_cfg,\n",
        "  )\n",
        "  return functools.partial(\n",
        "      train_lib.train,\n",
        "      model=get_t5_model(),\n",
        "      train_dataset_cfg=train_dataset_cfg,\n",
        "      train_eval_dataset_cfg=eval_dataset_cfg,\n",
        "      infer_eval_dataset_cfg=None,\n",
        "      checkpoint_cfg=ckpt_cfg,\n",
        "      partitioner=partitioner,\n",
        "      trainer_cls=trainer_cls,\n",
        "      total_steps=3,\n",
        "      eval_steps=2,\n",
        "      eval_period=1000,\n",
        "      random_seed=0,\n",
        "      xprof_seconds=0,  # GOOGLE-INTERNAL\n",
        "      summarize_config_fn=gin_utils.summarize_gin_config,\n",
        "      use_orbax=False,\n",
        "      gc_period=4,\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk7xKWTK6Ya9"
      },
      "outputs": [],
      "source": [
        "wmt_task = airio.examples.tasks.get_wmt_19_ende_v003_task()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUoBqy9f6cVx"
      },
      "outputs": [],
      "source": [
        "train_fn = create_train_fn(wmt_task)\n",
        "workdir = tempfile.mkdtemp()\n",
        "step, _ = train_fn(model_dir=workdir)\n",
        "print(f\"step: {step}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/airio/examples:colab_notebook_auto_kernel",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
